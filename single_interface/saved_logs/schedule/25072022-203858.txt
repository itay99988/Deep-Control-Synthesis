Chosen hyperparameters for training:
{'history_len': 2, 'timesteps_per_batch': 3000, 'max_timesteps_per_episode': 25, 'gamma': 0.99, 'n_updates_per_iteration': 10, 'lr': 0.01, 'clip': 0.2, 'total_timesteps': 1500000}
Training
Training from scratch.
Learning... Running 25 timesteps per episode, 3000 timesteps per batch for a total of 1500000 timesteps
tensor([0.5175, 0.4825], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #1 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -19.63
Average Loss: 0.15673
Timesteps So Far: 3000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.5472, 0.4528], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #2 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -19.57
Average Loss: 0.00075
Timesteps So Far: 6000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.5789, 0.4211], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #3 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -20.13
Average Loss: -0.00873
Timesteps So Far: 9000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.5796, 0.4204], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #4 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -20.57
Average Loss: -0.01861
Timesteps So Far: 12000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.5634, 0.4366], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #5 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -20.23
Average Loss: -0.05157
Timesteps So Far: 15000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.5381, 0.4619], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #6 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -19.2
Average Loss: -0.04966
Timesteps So Far: 18000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.5131, 0.4869], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #7 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -18.83
Average Loss: -0.0639
Timesteps So Far: 21000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.4764, 0.5236], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #8 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -18.63
Average Loss: -0.06815
Timesteps So Far: 24000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.4048, 0.5952], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #9 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -16.07
Average Loss: 0.00081
Timesteps So Far: 27000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.2859, 0.7141], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #10 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -11.67
Average Loss: 0.03911
Timesteps So Far: 30000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1421, 0.8579], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #11 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 0.7
Average Loss: 0.10942
Timesteps So Far: 33000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0644, 0.9356], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #12 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 8.13
Average Loss: 0.11727
Timesteps So Far: 36000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0377, 0.9623], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #13 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 14.8
Average Loss: 0.11512
Timesteps So Far: 39000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0311, 0.9689], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #14 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 13.8
Average Loss: 0.11569
Timesteps So Far: 42000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0249, 0.9751], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #15 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 17.87
Average Loss: 0.11031
Timesteps So Far: 45000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0226, 0.9774], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #16 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 18.23
Average Loss: 0.11014
Timesteps So Far: 48000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0190, 0.9810], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #17 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 19.7
Average Loss: 0.10507
Timesteps So Far: 51000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0166, 0.9834], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #18 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 19.03
Average Loss: 0.10781
Timesteps So Far: 54000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0143, 0.9857], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #19 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 20.9
Average Loss: 0.10392
Timesteps So Far: 57000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0124, 0.9876], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #20 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 19.73
Average Loss: 0.10249
Timesteps So Far: 60000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0108, 0.9892], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #21 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 22.53
Average Loss: 0.09773
Timesteps So Far: 63000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0100, 0.9900], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #22 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 22.57
Average Loss: 0.08023
Timesteps So Far: 66000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0120, 0.9880], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #23 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 21.4
Average Loss: 0.01399
Timesteps So Far: 69000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0181, 0.9819], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #24 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 19.97
Average Loss: -0.0269
Timesteps So Far: 72000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0080, 0.9920], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #25 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 22.93
Average Loss: -0.02657
Timesteps So Far: 75000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0024, 0.9976], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #26 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 24.37
Average Loss: -0.04197
Timesteps So Far: 78000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0013, 0.9987], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #27 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 24.53
Average Loss: -0.04385
Timesteps So Far: 81000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([8.4766e-04, 9.9915e-01], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #28 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 24.67
Average Loss: -0.04421
Timesteps So Far: 84000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([6.4391e-04, 9.9936e-01], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #29 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 24.07
Average Loss: -0.0446
Timesteps So Far: 87000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([5.3530e-04, 9.9946e-01], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #30 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 24.63
Average Loss: 0.00034
Timesteps So Far: 90000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([1.0886e-04, 9.9989e-01], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #31 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 25.0
Average Loss: -0.02162
Timesteps So Far: 93000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([5.6703e-05, 9.9994e-01], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #32 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 25.0
Average Loss: -0.01937
Timesteps So Far: 96000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([5.6359e-05, 9.9994e-01], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #33 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 25.0
Average Loss: -0.0245
Timesteps So Far: 99000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([3.2896e-05, 9.9997e-01], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #34 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 25.0
Average Loss: -0.02397
Timesteps So Far: 102000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([2.6508e-05, 9.9997e-01], grad_fn=<SoftmaxBackward0>)

Chosen hyperparameters for training:
{'history_len': 2, 'timesteps_per_batch': 2000, 'max_timesteps_per_episode': 25, 'gamma': 0.99, 'n_updates_per_iteration': 10, 'lr': 0.01, 'clip': 0.2, 'total_timesteps': 1500000}
Testing ppo_actor.pth

-------------------- Episode #0 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #1 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #2 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #3 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #4 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #5 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #6 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #7 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #8 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #9 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------

