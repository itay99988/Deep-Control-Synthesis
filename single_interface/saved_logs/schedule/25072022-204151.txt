Chosen hyperparameters for training:
{'history_len': 2, 'timesteps_per_batch': 3000, 'max_timesteps_per_episode': 25, 'gamma': 0.99, 'n_updates_per_iteration': 10, 'lr': 0.01, 'clip': 0.2, 'total_timesteps': 1500000}
Training
Training from scratch.
Learning... Running 25 timesteps per episode, 3000 timesteps per batch for a total of 1500000 timesteps
tensor([0.4501, 0.5499], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #1 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -18.17
Average Loss: 0.22089
Timesteps So Far: 3000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.4554, 0.5446], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #2 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -17.3
Average Loss: 0.10693
Timesteps So Far: 6000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.4472, 0.5528], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #3 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -18.57
Average Loss: -0.0084
Timesteps So Far: 9000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.4084, 0.5916], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #4 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -15.97
Average Loss: 0.01387
Timesteps So Far: 12000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.2909, 0.7091], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #5 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -11.67
Average Loss: 0.04645
Timesteps So Far: 15000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1454, 0.8546], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #6 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 5.07
Average Loss: 0.11817
Timesteps So Far: 18000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0802, 0.9198], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #7 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 17.83
Average Loss: 0.11165
Timesteps So Far: 21000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0700, 0.9300], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #8 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 15.57
Average Loss: 0.09655
Timesteps So Far: 24000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0501, 0.9499], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #9 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 18.4
Average Loss: 0.0817
Timesteps So Far: 27000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0185, 0.9815], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #10 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 24.57
Average Loss: 0.08508
Timesteps So Far: 30000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0088, 0.9912], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #11 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 24.37
Average Loss: 0.04845
Timesteps So Far: 33000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0015, 0.9985], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #12 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 25.0
Average Loss: 0.0133
Timesteps So Far: 36000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([2.3435e-04, 9.9977e-01], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #13 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 25.0
Average Loss: 0.00271
Timesteps So Far: 39000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([5.8905e-05, 9.9994e-01], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #14 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 25.0
Average Loss: -0.00511
Timesteps So Far: 42000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([3.2975e-05, 9.9997e-01], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #15 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 25.0
Average Loss: -0.00904
Timesteps So Far: 45000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([1.7493e-05, 9.9998e-01], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #16 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 25.0
Average Loss: -0.0105
Timesteps So Far: 48000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([1.8243e-05, 9.9998e-01], grad_fn=<SoftmaxBackward0>)

Chosen hyperparameters for training:
{'history_len': 2, 'timesteps_per_batch': 2000, 'max_timesteps_per_episode': 25, 'gamma': 0.99, 'n_updates_per_iteration': 10, 'lr': 0.01, 'clip': 0.2, 'total_timesteps': 1500000}
Testing ppo_actor.pth

-------------------- Episode #0 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #1 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #2 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #3 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #4 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #5 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #6 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #7 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #8 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #9 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------

