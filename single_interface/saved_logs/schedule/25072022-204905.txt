Chosen hyperparameters for training:
{'history_len': 2, 'timesteps_per_batch': 3000, 'max_timesteps_per_episode': 25, 'gamma': 0.99, 'n_updates_per_iteration': 10, 'lr': 0.01, 'clip': 0.2, 'total_timesteps': 1500000}
Training
Training from scratch.
Learning... Running 25 timesteps per episode, 3000 timesteps per batch for a total of 1500000 timesteps
tensor([0.5860, 0.4140], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #1 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -20.13
Average Loss: 0.17992
Timesteps So Far: 3000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.6120, 0.3880], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #2 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -20.47
Average Loss: 0.01296
Timesteps So Far: 6000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.6518, 0.3482], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #3 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -21.5
Average Loss: -0.01723
Timesteps So Far: 9000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.6612, 0.3388], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #4 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -20.97
Average Loss: -0.01839
Timesteps So Far: 12000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.6479, 0.3521], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #5 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -21.37
Average Loss: -0.0338
Timesteps So Far: 15000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.6233, 0.3767], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #6 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -21.0
Average Loss: -0.05957
Timesteps So Far: 18000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.6013, 0.3987], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #7 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -19.63
Average Loss: -0.05905
Timesteps So Far: 21000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.5894, 0.4106], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #8 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -20.5
Average Loss: -0.07363
Timesteps So Far: 24000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.5758, 0.4242], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #9 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -19.13
Average Loss: -0.05464
Timesteps So Far: 27000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.5593, 0.4407], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #10 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -19.67
Average Loss: -0.06411
Timesteps So Far: 30000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.5416, 0.4584], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #11 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -19.53
Average Loss: -0.07752
Timesteps So Far: 33000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.5343, 0.4657], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #12 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -19.9
Average Loss: -0.07192
Timesteps So Far: 36000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.5317, 0.4683], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #13 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -18.83
Average Loss: -0.04647
Timesteps So Far: 39000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.5189, 0.4811], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #14 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -19.67
Average Loss: -0.06389
Timesteps So Far: 42000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.5033, 0.4967], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #15 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -19.07
Average Loss: -0.04712
Timesteps So Far: 45000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.4855, 0.5145], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #16 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -19.5
Average Loss: -0.05712
Timesteps So Far: 48000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.4682, 0.5318], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #17 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -17.87
Average Loss: -0.02981
Timesteps So Far: 51000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.4436, 0.5564], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #18 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -18.93
Average Loss: -0.01568
Timesteps So Far: 54000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.4168, 0.5832], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #19 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -17.83
Average Loss: 0.0378
Timesteps So Far: 57000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.3809, 0.6191], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #20 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -17.1
Average Loss: 0.0538
Timesteps So Far: 60000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.3407, 0.6593], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #21 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -15.03
Average Loss: 0.09527
Timesteps So Far: 63000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.2875, 0.7125], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #22 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -12.7
Average Loss: 0.0799
Timesteps So Far: 66000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.2168, 0.7832], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #23 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -3.77
Average Loss: 0.0972
Timesteps So Far: 69000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1325, 0.8675], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #24 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 8.9
Average Loss: 0.09194
Timesteps So Far: 72000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0642, 0.9358], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #25 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 19.23
Average Loss: 0.089
Timesteps So Far: 75000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0250, 0.9750], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #26 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 23.27
Average Loss: 0.09549
Timesteps So Far: 78000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0076, 0.9924], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #27 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 24.63
Average Loss: 0.09001
Timesteps So Far: 81000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0031, 0.9969], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #28 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 25.0
Average Loss: 0.07454
Timesteps So Far: 84000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0018, 0.9982], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #29 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 25.0
Average Loss: 0.0549
Timesteps So Far: 87000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0011, 0.9989], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #30 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 24.6
Average Loss: 0.03449
Timesteps So Far: 90000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([8.1490e-04, 9.9919e-01], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #31 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 25.0
Average Loss: 0.02144
Timesteps So Far: 93000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([6.5605e-04, 9.9934e-01], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #32 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 25.0
Average Loss: 0.00952
Timesteps So Far: 96000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([7.2570e-04, 9.9927e-01], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #33 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 25.0
Average Loss: 0.00135
Timesteps So Far: 99000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([6.2030e-04, 9.9938e-01], grad_fn=<SoftmaxBackward0>)

Chosen hyperparameters for training:
{'history_len': 2, 'timesteps_per_batch': 2000, 'max_timesteps_per_episode': 25, 'gamma': 0.99, 'n_updates_per_iteration': 10, 'lr': 0.01, 'clip': 0.2, 'total_timesteps': 1500000}
Testing ppo_actor.pth

-------------------- Episode #0 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #1 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #2 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #3 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #4 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #5 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #6 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #7 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #8 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------


-------------------- Episode #9 --------------------
Episodic Length: 200
Episodic Return: 200
Failure Rate: 0.0%
------------------------------------------------------

