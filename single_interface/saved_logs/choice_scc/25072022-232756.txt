Chosen hyperparameters for training:
{'history_len': 2, 'timesteps_per_batch': 3000, 'max_timesteps_per_episode': 25, 'gamma': 0.99, 'n_updates_per_iteration': 10, 'lr': 0.01, 'clip': 0.2, 'total_timesteps': 1500000}
Training
Training from scratch.
Learning... Running 25 timesteps per episode, 3000 timesteps per batch for a total of 1500000 timesteps
tensor([0.2740, 0.2026, 0.2727, 0.2507], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #1 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 2.3
Average Loss: 0.20335
Timesteps So Far: 3000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.2808, 0.1984, 0.2625, 0.2583], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #2 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 2.3
Average Loss: 0.14077
Timesteps So Far: 6000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.2815, 0.1909, 0.2398, 0.2878], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #3 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 1.1
Average Loss: 0.02197
Timesteps So Far: 9000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.2772, 0.1756, 0.2061, 0.3410], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #4 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 3.4
Average Loss: 0.01142
Timesteps So Far: 12000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.2432, 0.1573, 0.1851, 0.4144], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #5 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 3.1
Average Loss: 0.01671
Timesteps So Far: 15000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.2079, 0.1409, 0.1757, 0.4755], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #6 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 4.5
Average Loss: 0.01942
Timesteps So Far: 18000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1828, 0.1232, 0.1627, 0.5313], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #7 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 9.4
Average Loss: 0.04049
Timesteps So Far: 21000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1324, 0.0909, 0.1281, 0.6487], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #8 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 11.5
Average Loss: 0.034
Timesteps So Far: 24000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1156, 0.0779, 0.1133, 0.6931], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #9 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 12.7
Average Loss: 0.01369
Timesteps So Far: 27000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0980, 0.0669, 0.1005, 0.7346], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #10 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 11.8
Average Loss: -0.01623
Timesteps So Far: 30000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0882, 0.0603, 0.0923, 0.7592], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #11 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 13.9
Average Loss: -0.05149
Timesteps So Far: 33000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0676, 0.0466, 0.0724, 0.8134], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #12 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 15.6
Average Loss: -0.07274
Timesteps So Far: 36000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0440, 0.0308, 0.0476, 0.8775], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #13 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 16.5
Average Loss: -0.08483
Timesteps So Far: 39000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0249, 0.0177, 0.0267, 0.9307], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #14 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 16.9
Average Loss: -0.08447
Timesteps So Far: 42000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0156, 0.0111, 0.0163, 0.9570], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #15 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 18.6
Average Loss: -0.08031
Timesteps So Far: 45000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0108, 0.0077, 0.0110, 0.9704], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #16 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 18.8
Average Loss: -0.07584
Timesteps So Far: 48000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0083, 0.0059, 0.0084, 0.9774], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #17 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 17.9
Average Loss: -0.07304
Timesteps So Far: 51000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0067, 0.0048, 0.0067, 0.9819], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #18 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 19.0
Average Loss: -0.07586
Timesteps So Far: 54000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0056, 0.0040, 0.0056, 0.9848], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #19 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 18.7
Average Loss: -0.07746
Timesteps So Far: 57000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0049, 0.0035, 0.0049, 0.9867], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #20 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 18.5
Average Loss: -0.0761
Timesteps So Far: 60000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0045, 0.0032, 0.0044, 0.9879], grad_fn=<SoftmaxBackward0>)

Chosen hyperparameters for training:
{'history_len': 2, 'timesteps_per_batch': 2000, 'max_timesteps_per_episode': 25, 'gamma': 0.99, 'n_updates_per_iteration': 10, 'lr': 0.01, 'clip': 0.2, 'total_timesteps': 1500000}
Testing ppo_actor.pth

-------------------- Episode #0 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #1 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #2 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #3 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #4 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #5 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #6 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #7 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #8 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #9 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------

