Chosen hyperparameters for training:
{'history_len': 2, 'timesteps_per_batch': 3000, 'max_timesteps_per_episode': 25, 'gamma': 0.99, 'n_updates_per_iteration': 10, 'lr': 0.01, 'clip': 0.2, 'total_timesteps': 1500000}
Training
Training from scratch.
Learning... Running 25 timesteps per episode, 3000 timesteps per batch for a total of 1500000 timesteps
tensor([0.2111, 0.3001, 0.2208, 0.2680], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #1 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 2.2
Average Loss: 0.24047
Timesteps So Far: 3000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.2138, 0.2838, 0.2119, 0.2905], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #2 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 1.2
Average Loss: 0.19706
Timesteps So Far: 6000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.2228, 0.2997, 0.2157, 0.2619], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #3 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 3.2
Average Loss: 0.05921
Timesteps So Far: 9000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.2326, 0.3341, 0.2208, 0.2126], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #4 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 2.6
Average Loss: 0.01478
Timesteps So Far: 12000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.2299, 0.3407, 0.2199, 0.2095], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #5 --------------------
Average Episodic Length: 25.0
Average Episodic Return: -0.8
Average Loss: 0.00959
Timesteps So Far: 15000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.2211, 0.3233, 0.2152, 0.2403], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #6 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 1.8
Average Loss: 0.01501
Timesteps So Far: 18000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.2091, 0.2923, 0.2055, 0.2931], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #7 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 2.6
Average Loss: -0.00161
Timesteps So Far: 21000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1964, 0.2575, 0.1922, 0.3540], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #8 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 4.6
Average Loss: -0.00618
Timesteps So Far: 24000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1829, 0.2416, 0.1825, 0.3930], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #9 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 6.0
Average Loss: -0.02163
Timesteps So Far: 27000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1709, 0.2289, 0.1773, 0.4228], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #10 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 5.8
Average Loss: -0.02757
Timesteps So Far: 30000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1617, 0.2186, 0.1711, 0.4485], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #11 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 5.5
Average Loss: -0.06169
Timesteps So Far: 33000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1547, 0.2110, 0.1658, 0.4685], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #12 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 6.2
Average Loss: -0.04761
Timesteps So Far: 36000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1425, 0.1947, 0.1508, 0.5119], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #13 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 7.4
Average Loss: -0.04334
Timesteps So Far: 39000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1328, 0.1822, 0.1401, 0.5448], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #14 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 7.1
Average Loss: -0.0579
Timesteps So Far: 42000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1241, 0.1712, 0.1314, 0.5733], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #15 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 9.4
Average Loss: -0.04471
Timesteps So Far: 45000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1099, 0.1529, 0.1160, 0.6212], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #16 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 9.4
Average Loss: -0.04348
Timesteps So Far: 48000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0942, 0.1307, 0.0981, 0.6770], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #17 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 11.8
Average Loss: -0.04047
Timesteps So Far: 51000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0627, 0.0846, 0.0631, 0.7897], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #18 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 14.0
Average Loss: -0.03355
Timesteps So Far: 54000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0265, 0.0311, 0.0255, 0.9169], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #19 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 16.5
Average Loss: -0.03722
Timesteps So Far: 57000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0104, 0.0115, 0.0096, 0.9685], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #20 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 18.6
Average Loss: -0.03533
Timesteps So Far: 60000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0044, 0.0047, 0.0039, 0.9870], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #21 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 18.3
Average Loss: -0.03016
Timesteps So Far: 63000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0028, 0.0029, 0.0024, 0.9919], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #22 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 19.0
Average Loss: -0.03056
Timesteps So Far: 66000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0019, 0.0020, 0.0016, 0.9945], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #23 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 19.0
Average Loss: -0.02794
Timesteps So Far: 69000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0015, 0.0015, 0.0013, 0.9958], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #24 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 19.0
Average Loss: -0.03103
Timesteps So Far: 72000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0013, 0.0013, 0.0011, 0.9964], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #25 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 18.7
Average Loss: -0.02258
Timesteps So Far: 75000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([1.0265e-03, 1.0429e-03, 8.7239e-04, 9.9706e-01],
       grad_fn=<SoftmaxBackward0>)

Chosen hyperparameters for training:
{'history_len': 2, 'timesteps_per_batch': 2000, 'max_timesteps_per_episode': 25, 'gamma': 0.99, 'n_updates_per_iteration': 10, 'lr': 0.01, 'clip': 0.2, 'total_timesteps': 1500000}
Testing ppo_actor.pth

-------------------- Episode #0 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #1 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #2 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #3 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #4 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #5 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #6 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #7 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #8 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #9 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------

