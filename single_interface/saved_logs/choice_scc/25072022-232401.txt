Chosen hyperparameters for training:
{'history_len': 2, 'timesteps_per_batch': 3000, 'max_timesteps_per_episode': 25, 'gamma': 0.99, 'n_updates_per_iteration': 10, 'lr': 0.01, 'clip': 0.2, 'total_timesteps': 1500000}
Training
Training from scratch.
Learning... Running 25 timesteps per episode, 3000 timesteps per batch for a total of 1500000 timesteps
tensor([0.2565, 0.2113, 0.3019, 0.2303], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #1 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 2.4
Average Loss: 0.21991
Timesteps So Far: 3000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.2314, 0.2085, 0.3112, 0.2488], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #2 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 1.9
Average Loss: 0.16072
Timesteps So Far: 6000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.2048, 0.1990, 0.3345, 0.2617], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #3 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 2.4
Average Loss: 0.01722
Timesteps So Far: 9000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1746, 0.1825, 0.3673, 0.2756], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #4 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 3.2
Average Loss: 0.01929
Timesteps So Far: 12000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1618, 0.1717, 0.3671, 0.2994], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #5 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 3.4
Average Loss: 0.03246
Timesteps So Far: 15000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1579, 0.1651, 0.3529, 0.3241], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #6 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 3.7
Average Loss: 0.03713
Timesteps So Far: 18000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1480, 0.1544, 0.3424, 0.3552], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #7 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 4.9
Average Loss: 0.03317
Timesteps So Far: 21000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1374, 0.1422, 0.3228, 0.3976], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #8 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 4.4
Average Loss: 0.00712
Timesteps So Far: 24000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1323, 0.1342, 0.2956, 0.4379], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #9 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 8.2
Average Loss: -0.01659
Timesteps So Far: 27000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1289, 0.1310, 0.2958, 0.4442], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #10 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 6.4
Average Loss: -0.03493
Timesteps So Far: 30000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1317, 0.1323, 0.2921, 0.4439], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #11 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 7.0
Average Loss: -0.04016
Timesteps So Far: 33000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1382, 0.1348, 0.2776, 0.4494], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #12 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 7.7
Average Loss: -0.04416
Timesteps So Far: 36000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1452, 0.1372, 0.2603, 0.4573], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #13 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 7.8
Average Loss: -0.03548
Timesteps So Far: 39000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.1413, 0.1319, 0.2409, 0.4859], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #14 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 9.6
Average Loss: 0.03393
Timesteps So Far: 42000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0925, 0.0904, 0.1807, 0.6364], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #15 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 11.4
Average Loss: 0.03095
Timesteps So Far: 45000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0416, 0.0440, 0.1089, 0.8055], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #16 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 13.8
Average Loss: 0.01273
Timesteps So Far: 48000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0298, 0.0323, 0.0820, 0.8559], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #17 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 15.8
Average Loss: -0.00216
Timesteps So Far: 51000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0329, 0.0352, 0.0800, 0.8519], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #18 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 16.9
Average Loss: -0.01673
Timesteps So Far: 54000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0325, 0.0345, 0.0713, 0.8617], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #19 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 15.0
Average Loss: -0.02074
Timesteps So Far: 57000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0291, 0.0310, 0.0602, 0.8797], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #20 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 16.9
Average Loss: -0.02994
Timesteps So Far: 60000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0192, 0.0211, 0.0412, 0.9185], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #21 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 16.9
Average Loss: -0.02798
Timesteps So Far: 63000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0111, 0.0126, 0.0256, 0.9507], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #22 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 18.0
Average Loss: -0.02917
Timesteps So Far: 66000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0059, 0.0070, 0.0151, 0.9719], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #23 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 18.7
Average Loss: -0.03196
Timesteps So Far: 69000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0038, 0.0046, 0.0102, 0.9814], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #24 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 19.0
Average Loss: -0.03212
Timesteps So Far: 72000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0025, 0.0031, 0.0072, 0.9872], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #25 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 18.5
Average Loss: -0.02009
Timesteps So Far: 75000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0019, 0.0024, 0.0057, 0.9900], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #26 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 18.6
Average Loss: -0.01693
Timesteps So Far: 78000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0015, 0.0019, 0.0045, 0.9921], grad_fn=<SoftmaxBackward0>)

-------------------- Iteration #27 --------------------
Average Episodic Length: 25.0
Average Episodic Return: 18.9
Average Loss: -0.01974
Timesteps So Far: 81000
Iteration took: 0.0 secs
------------------------------------------------------

tensor([0.0013, 0.0017, 0.0041, 0.9928], grad_fn=<SoftmaxBackward0>)

Chosen hyperparameters for training:
{'history_len': 2, 'timesteps_per_batch': 2000, 'max_timesteps_per_episode': 25, 'gamma': 0.99, 'n_updates_per_iteration': 10, 'lr': 0.01, 'clip': 0.2, 'total_timesteps': 1500000}
Testing ppo_actor.pth

-------------------- Episode #0 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #1 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #2 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #3 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #4 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #5 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #6 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #7 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #8 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------


-------------------- Episode #9 --------------------
Episodic Length: 200
Episodic Return: 194
Failure Rate: 1.5%
------------------------------------------------------

